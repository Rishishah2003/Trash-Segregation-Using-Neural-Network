{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imutils\n",
    "from imutils import paths\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "images_med = []\n",
    "def extract_images_from_folder(folder_path, target_size):\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, target_size)  # Resize the image to a consistent shape\n",
    "            images_med.append(img)\n",
    "        else:\n",
    "            print(f\"Could not read the image at {img_path}\")\n",
    "\n",
    "# Example usage:\n",
    "folder_path = 'C:/Users/Rishi/Desktop/New folder/TrashBox_train_set/medical'  # Replace with the path to your image folder\n",
    "target_size = (256, 256)  # Set your target size for resizing\n",
    "image_array = extract_images_from_folder(folder_path, target_size)\n",
    "\n",
    "# Now you can use the 'image_array' to access the images and perform operations on them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_med= np.array(images_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1557, 256, 256, 3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_med.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "images_card = []\n",
    "def extract_images_from_folder(folder_path, target_size):\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, target_size)  # Resize the image to a consistent shape\n",
    "            images_card.append(img)\n",
    "        else:\n",
    "            print(f\"Could not read the image at {img_path}\")\n",
    "\n",
    "# Example usage:\n",
    "folder_path = 'C:/Users/Rishi/Desktop/New folder/TrashBox_train_set/cardboard'  # Replace with the path to your image folder\n",
    "target_size = (256, 256)  # Set your target size for resizing\n",
    "image_array = extract_images_from_folder(folder_path, target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_card= np.array(images_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1877, 256, 256, 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_card.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "images_ewaste = []\n",
    "def extract_images_from_folder(folder_path, target_size):\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, target_size)  # Resize the image to a consistent shape\n",
    "            images_ewaste.append(img)\n",
    "        else:\n",
    "            print(f\"Could not read the image at {img_path}\")\n",
    "\n",
    "# Example usage:\n",
    "folder_path = 'C:/Users/Rishi/Desktop/New folder/TrashBox_train_set/e-waste'  # Replace with the path to your image folder\n",
    "target_size = (256, 256)  # Set your target size for resizing\n",
    "image_array = extract_images_from_folder(folder_path, target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_ewaste= np.array(images_ewaste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2308, 256, 256, 3)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_ewaste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "images_glass = []\n",
    "def extract_images_from_folder(folder_path, target_size):\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, target_size)  # Resize the image to a consistent shape\n",
    "            images_glass.append(img)\n",
    "        else:\n",
    "            print(f\"Could not read the image at {img_path}\")\n",
    "\n",
    "# Example usage:\n",
    "folder_path = 'C:/Users/Rishi/Desktop/New folder/TrashBox_train_set/glass'  # Replace with the path to your image folder\n",
    "target_size = (256, 256)  # Set your target size for resizing\n",
    "image_array = extract_images_from_folder(folder_path, target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_glass= np.array(images_glass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2017, 256, 256, 3)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_glass.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "images_metal = []\n",
    "def extract_images_from_folder(folder_path, target_size):\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, target_size)  # Resize the image to a consistent shape\n",
    "            images_metal.append(img)\n",
    "        else:\n",
    "            print(f\"Could not read the image at {img_path}\")\n",
    "\n",
    "# Example usage:\n",
    "folder_path = 'C:/Users/Rishi/Desktop/New folder/TrashBox_train_set/metal'  # Replace with the path to your image folder\n",
    "target_size = (256, 256)  # Set your target size for resizing\n",
    "image_array = extract_images_from_folder(folder_path, target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_metal= np.array(images_metal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2066, 256, 256, 3)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_metal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "images_paper = []\n",
    "def extract_images_from_folder(folder_path, target_size):\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, target_size)  # Resize the image to a consistent shape\n",
    "            images_paper.append(img)\n",
    "        else:\n",
    "            print(f\"Could not read the image at {img_path}\")\n",
    "\n",
    "# Example usage:\n",
    "folder_path = 'C:/Users/Rishi/Desktop/New folder/TrashBox_train_set/paper'  # Replace with the path to your image folder\n",
    "target_size = (256, 256)  # Set your target size for resizing\n",
    "image_array = extract_images_from_folder(folder_path, target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_paper= np.array(images_paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2151, 256, 256, 3)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_paper.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "images_plastic = []\n",
    "def extract_images_from_folder(folder_path, target_size):\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, target_size)  # Resize the image to a consistent shape\n",
    "            images_plastic.append(img)\n",
    "        else:\n",
    "            print(f\"Could not read the image at {img_path}\")\n",
    "\n",
    "# Example usage:\n",
    "folder_path = 'C:/Users/Rishi/Desktop/New folder/TrashBox_train_set/plastic'  # Replace with the path to your image folder\n",
    "target_size = (256, 256)  # Set your target size for resizing\n",
    "image_array = extract_images_from_folder(folder_path, target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_plastic= np.array(images_plastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(428, 256, 256, 3)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_plastic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "images= np.concatenate((images_card,images_ewaste,images_glass,images_med,images_metal,images_paper,images_plastic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1= np.array(np.repeat(0,1877))\n",
    "y2= np.array(np.repeat(1,2308))\n",
    "y3= np.array(np.repeat(2,2017))\n",
    "y4= np.array(np.repeat(3,1557))\n",
    "y5= np.array(np.repeat(4,2066))\n",
    "y6= np.array(np.repeat(5,2151))\n",
    "y7= np.array(np.repeat(6,428))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= np.concatenate((y1,y2,y3,y4,y5,y6,y7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= pd.get_dummies(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12404, 7)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12399</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12400</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12401</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12402</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12403</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12404 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0      1      2      3      4      5      6\n",
       "0       True  False  False  False  False  False  False\n",
       "1       True  False  False  False  False  False  False\n",
       "2       True  False  False  False  False  False  False\n",
       "3       True  False  False  False  False  False  False\n",
       "4       True  False  False  False  False  False  False\n",
       "...      ...    ...    ...    ...    ...    ...    ...\n",
       "12399  False  False  False  False  False  False   True\n",
       "12400  False  False  False  False  False  False   True\n",
       "12401  False  False  False  False  False  False   True\n",
       "12402  False  False  False  False  False  False   True\n",
       "12403  False  False  False  False  False  False   True\n",
       "\n",
       "[12404 rows x 7 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "images= images[:,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12404, 256, 256)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "images= images.reshape((12404,256*256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "25/25 [==============================] - 33s 1s/step - loss: 16457.3848 - accuracy: 0.1506\n",
      "Epoch 2/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 3214.3127 - accuracy: 0.1645\n",
      "Epoch 3/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 1132.3351 - accuracy: 0.1737\n",
      "Epoch 4/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 633.6298 - accuracy: 0.1824\n",
      "Epoch 5/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 559.3531 - accuracy: 0.1695\n",
      "Epoch 6/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 503.6414 - accuracy: 0.1772\n",
      "Epoch 7/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 297.8658 - accuracy: 0.1828\n",
      "Epoch 8/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 180.3921 - accuracy: 0.1754\n",
      "Epoch 9/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 132.7463 - accuracy: 0.1855\n",
      "Epoch 10/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 177.9958 - accuracy: 0.1823\n",
      "Epoch 11/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 186.5478 - accuracy: 0.1727\n",
      "Epoch 12/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 134.0432 - accuracy: 0.1620\n",
      "Epoch 13/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 96.4210 - accuracy: 0.1791\n",
      "Epoch 14/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 50.4140 - accuracy: 0.1811\n",
      "Epoch 15/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 47.7677 - accuracy: 0.1737\n",
      "Epoch 16/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 42.1448 - accuracy: 0.1860\n",
      "Epoch 17/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 31.6576 - accuracy: 0.1950\n",
      "Epoch 18/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 48.2893 - accuracy: 0.1773\n",
      "Epoch 19/60\n",
      "25/25 [==============================] - 32s 1s/step - loss: 37.5781 - accuracy: 0.1767\n",
      "Epoch 20/60\n",
      "25/25 [==============================] - 30s 1s/step - loss: 33.5969 - accuracy: 0.1654\n",
      "Epoch 21/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 45.8990 - accuracy: 0.1667\n",
      "Epoch 22/60\n",
      "25/25 [==============================] - 30s 1s/step - loss: 66.2278 - accuracy: 0.1637\n",
      "Epoch 23/60\n",
      "25/25 [==============================] - 30s 1s/step - loss: 58.1520 - accuracy: 0.1650\n",
      "Epoch 24/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 55.3730 - accuracy: 0.1774\n",
      "Epoch 25/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 31.9909 - accuracy: 0.1737\n",
      "Epoch 26/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 41.1012 - accuracy: 0.1639\n",
      "Epoch 27/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 41.9034 - accuracy: 0.1562\n",
      "Epoch 28/60\n",
      "25/25 [==============================] - 30s 1s/step - loss: 20.6520 - accuracy: 0.1733\n",
      "Epoch 29/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 12.3417 - accuracy: 0.1815\n",
      "Epoch 30/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 7.1836 - accuracy: 0.1779\n",
      "Epoch 31/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 3.4575 - accuracy: 0.1891\n",
      "Epoch 32/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 2.2259 - accuracy: 0.2032\n",
      "Epoch 33/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 1.9230 - accuracy: 0.2261\n",
      "Epoch 34/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 1.8898 - accuracy: 0.2259\n",
      "Epoch 35/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 1.8447 - accuracy: 0.2390\n",
      "Epoch 36/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 1.8244 - accuracy: 0.2456\n",
      "Epoch 37/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 1.8012 - accuracy: 0.2542\n",
      "Epoch 38/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 1.8328 - accuracy: 0.2485\n",
      "Epoch 39/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 1.8066 - accuracy: 0.2535\n",
      "Epoch 40/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 1.8040 - accuracy: 0.2622\n",
      "Epoch 41/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 1.8662 - accuracy: 0.2502\n",
      "Epoch 42/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 2.1106 - accuracy: 0.2186\n",
      "Epoch 43/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 2.0151 - accuracy: 0.2229\n",
      "Epoch 44/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 1.8233 - accuracy: 0.2596\n",
      "Epoch 45/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 1.7721 - accuracy: 0.2718\n",
      "Epoch 46/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 1.8912 - accuracy: 0.2534\n",
      "Epoch 47/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 2.0930 - accuracy: 0.2414\n",
      "Epoch 48/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 20.6266 - accuracy: 0.1652\n",
      "Epoch 49/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 27.4837 - accuracy: 0.1645\n",
      "Epoch 50/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 24.0411 - accuracy: 0.1640\n",
      "Epoch 51/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 63.5233 - accuracy: 0.1580\n",
      "Epoch 52/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 66.6944 - accuracy: 0.1582\n",
      "Epoch 53/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 5.4261 - accuracy: 0.1601\n",
      "Epoch 54/60\n",
      "25/25 [==============================] - 30s 1s/step - loss: 1.9393 - accuracy: 0.1618\n",
      "Epoch 55/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 1.9158 - accuracy: 0.1654\n",
      "Epoch 56/60\n",
      "25/25 [==============================] - 30s 1s/step - loss: 1.9046 - accuracy: 0.1654\n",
      "Epoch 57/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 1.8939 - accuracy: 0.1658\n",
      "Epoch 58/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 1.8838 - accuracy: 0.1665\n",
      "Epoch 59/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 1.8771 - accuracy: 0.1665\n",
      "Epoch 60/60\n",
      "25/25 [==============================] - 31s 1s/step - loss: 1.8727 - accuracy: 0.1665\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x231223b88d0>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(1080,activation='relu',input_shape=(256*256,)))      #can take input_shape= (X.shape[1],X.shape[0])\n",
    "\n",
    "model.add(Dense(541,activation='relu'))\n",
    "\n",
    "model.add(Dense(64,activation='LeakyReLU'))\n",
    "\n",
    "model.add(Dense(7,activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(images, y, epochs=60, batch_size=500)  # Adjust epochs and batch_size as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "311/311 [==============================] - 577s 2s/step - loss: 1.9006 - accuracy: 0.2047 - val_loss: 1.8176 - val_accuracy: 0.2543\n",
      "Epoch 2/10\n",
      "311/311 [==============================] - 581s 2s/step - loss: 1.7845 - accuracy: 0.2759 - val_loss: 1.7241 - val_accuracy: 0.2950\n",
      "Epoch 3/10\n",
      "311/311 [==============================] - 412s 1s/step - loss: 1.6712 - accuracy: 0.3452 - val_loss: 1.6371 - val_accuracy: 0.3813\n",
      "Epoch 4/10\n",
      "311/311 [==============================] - 417s 1s/step - loss: 1.5279 - accuracy: 0.4144 - val_loss: 1.6024 - val_accuracy: 0.4079\n",
      "Epoch 5/10\n",
      "311/311 [==============================] - 400s 1s/step - loss: 1.3481 - accuracy: 0.4911 - val_loss: 1.6344 - val_accuracy: 0.4281\n",
      "Epoch 6/10\n",
      "311/311 [==============================] - 405s 1s/step - loss: 1.1648 - accuracy: 0.5659 - val_loss: 1.6717 - val_accuracy: 0.4353\n",
      "Epoch 7/10\n",
      "311/311 [==============================] - 398s 1s/step - loss: 0.9801 - accuracy: 0.6331 - val_loss: 1.7892 - val_accuracy: 0.4397\n",
      "Epoch 8/10\n",
      "311/311 [==============================] - 419s 1s/step - loss: 0.8335 - accuracy: 0.6894 - val_loss: 1.8846 - val_accuracy: 0.4410\n",
      "Epoch 9/10\n",
      "311/311 [==============================] - 421s 1s/step - loss: 0.7241 - accuracy: 0.7240 - val_loss: 1.9477 - val_accuracy: 0.4482\n",
      "Epoch 10/10\n",
      "311/311 [==============================] - 421s 1s/step - loss: 0.6406 - accuracy: 0.7562 - val_loss: 2.1498 - val_accuracy: 0.4462\n",
      "78/78 [==============================] - 25s 313ms/step - loss: 2.1498 - accuracy: 0.4462\n",
      "Test accuracy: 0.44619104266166687\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Normalize the pixel values\n",
    "images = images / 255.0\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape the data for CNN input\n",
    "X_train = X_train.reshape(X_train.shape[0], 256, 256, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 256, 256, 1)\n",
    "\n",
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the convolutional layers\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Flatten the data and add dense layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = r\"C:\\Users\\Rishi\\Desktop\\images.jpeg\"\n",
    "img = load_img(test_image_path, target_size=(256, 256))  # Adjust target size as needed\n",
    "img_array = img_to_array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256, 3)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_pred=img_array[:,:,0]\n",
    "img_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pred=img_pred.reshape((1,256,256,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 62ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(img_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "from imutils.video import VideoStream\n",
    "import numpy as np\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "import os"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
